{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week15_NeuralNetworkForRegression",
      "provenance": [],
      "authorship_tag": "ABX9TyOVEXRwByFQ407LBtuMFF0L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ch00226855/CMP414765Spring2021/blob/main/Week15_NeuralNetworkForRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNStU6Yu6MuT"
      },
      "source": [
        "# Week 15\n",
        "# Deep Learning for Regression\n",
        "\n",
        "So far, we have seen how neural networks are used for classification tasks, such as image classification, text classification, text generation. Today we will apply the neural network model to a regression task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEUV4lTO6Ufh"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-91AOR37DtH"
      },
      "source": [
        "## Load Auto MPG Dataset\n",
        "\n",
        "This dataset is available from the [Kaggle.com](https://www.kaggle.com/uciml/autompg-dataset)\n",
        "\n",
        "**Please follow the steps below to download this dataset to the Colab environment:**\n",
        "\n",
        "1. Go to your Kaggle account, Scroll to API section and Click **Expire API Token** to remove previous tokens.\n",
        "\n",
        "2. Click on **Create New API Token** - It will download `kaggle.json` file on your machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URIqe5RY7kAo"
      },
      "source": [
        "# 3. Install the kaggle API\n",
        "! pip install kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJegD9KK7I_t"
      },
      "source": [
        "# 4. Upload the kaggle.json file\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UGgxoX87jJP"
      },
      "source": [
        "# 5. Make a directory named kaggle and copy kaggle.json file there\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Change the permission of the file\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlmA3-3x7mPS"
      },
      "source": [
        "# 6. Download and unzip the dataset\n",
        "!kaggle datasets download -d uciml/autompg-dataset\n",
        "!unzip autompg-dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9LiAGI56ZI_"
      },
      "source": [
        "## Attribute Information\n",
        "\n",
        "1. mpg: continuous\n",
        "2. cylinders: multi-valued discrete\n",
        "3. displacement: continuous\n",
        "4. horsepower: continuous\n",
        "5. weight: continuous\n",
        "6. acceleration: continuous\n",
        "7. model year: multi-valued discrete\n",
        "8. origin: multi-valued discrete\n",
        "9. car name: string (unique for each instance)\n",
        "\n",
        "The goal of this project is to train a model that predicts \"MPG\" using other features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfP8zjDj6a6A"
      },
      "source": [
        "# Load the dataset\n",
        "filename = 'auto-mpg.csv'\n",
        "dataset = pd.read_csv(filename,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\",\", skipinitialspace=True)\n",
        "\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U6EBsru9swP"
      },
      "source": [
        "# Drop the \"car name\" column, as the model tries to find the general relationship between mpg and other factors.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axiR31D06dH1"
      },
      "source": [
        "## Data Preprocessing\n",
        "- Check for missing values\n",
        "- Handle categorical features\n",
        "- Split into training and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hiRQY3W6fWc"
      },
      "source": [
        "# Are there any missing values?\n",
        "# If so, how should we handle them?\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGOI-DuW6hKl"
      },
      "source": [
        "# Consider using mean or median to fill the missing values\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGCRqkIS6kxv"
      },
      "source": [
        "# The \"Origin\" column is really categorical, not numeric. \n",
        "dataset['origin'] = dataset['origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgkOQqUp9fzR"
      },
      "source": [
        "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N18hB9W6rza"
      },
      "source": [
        "# Split the data into df_train (60%), df_validation (20%), and df_test (20%)\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePRVWv5W8STq"
      },
      "source": [
        "## Inspect the Data\n",
        "- Inspect the distribution of each individual feature:\n",
        "    - descriptive statistics\n",
        "    - histogram\n",
        "- Inspect the relationship between features:\n",
        "    - correlation coefficients\n",
        "    - scatter plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg9KOJEg8Tzb"
      },
      "source": [
        "# Compute descriptive statistices: min, max, mean, median, standard deviation\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbW-DY8Q8V9y"
      },
      "source": [
        "# Plot histogram and pairwise scatter plots for numerical variables\n",
        "pd.plotting.scatter_matrix(df_train, figsize=(15, 15))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcoiILT-8dLf"
      },
      "source": [
        "# Calculate the correlation coefficient between MPG and every other feature\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtiaUnTd82yk"
      },
      "source": [
        "## Build the Model\n",
        "- Transform data into proper format\n",
        "- Normalize data\n",
        "- Build neural network\n",
        "- Specify loss function and training method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktZZhvFH9B3V"
      },
      "source": [
        "# Separate the \"MPG\" feature from the rest.\n",
        "df_train_labels = df_train['mpg']\n",
        "# df_train_data = df_train.loc[:, ['Cylinders', 'Displacement',...]]\n",
        "df_train_data = df_train.iloc[:, 1:]\n",
        "df_train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe5YlsTM9MHl"
      },
      "source": [
        "# Perform this transformation to the validation set and the test set.\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2yWKTQf9WLJ"
      },
      "source": [
        "# Normalize the data by applying the following tranformation:\n",
        "# x <-- (x - mean) / std\n",
        "# Result of this transform:\n",
        "#   1. the average value of each column will be 0.\n",
        "#   2. the standard deviation of each column will be 1.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df_train_scaled = scaler.fit_transform(df_train_data)\n",
        "df_train_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcM--wki9ZYT"
      },
      "source": [
        "# Build a 3-layer neural network:\n",
        "# 1. input layer (what is the input shape?)\n",
        "# 2. hidden layer with 64 nodes and ReLU activation\n",
        "# 3. output layer (what is the output shape?)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBPCEWue-XXM"
      },
      "source": [
        "# Display a summary of the model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVcoTkYe-X62"
      },
      "source": [
        "# Use model.compile() to specify:\n",
        "# 1. loss = 'mse'\n",
        "# 2. optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "# 3. metrics = ['mae', 'mse']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4bewakB-Zfu"
      },
      "source": [
        "## Train the Model\n",
        "- Train the model\n",
        "- Analyze the loss curve\n",
        "- Improve the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kbHbQtO-a1Y"
      },
      "source": [
        "EPOCHS = 1000\n",
        "\n",
        "history = model.fit(\n",
        "  df_train_scaled, df_train_labels,\n",
        "  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt5gOYNB-cnK"
      },
      "source": [
        "# Visualize the model's training progress using the statistics stored in\n",
        "# the \"history\" object\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSyS0kZW-gJo"
      },
      "source": [
        "# Plot training MAE and validation MAE against epochs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3yoQ23P-hku"
      },
      "source": [
        "# Plot training MSE and validation MSE against epochs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT9JF_8--lyI"
      },
      "source": [
        "## Evaluate the Model\n",
        "- Evaluate the model on the test set\n",
        "- Visualize the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "debGGI9e-sXc"
      },
      "source": [
        "# Evalute the performance on the test set\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SXVFwW5-uUL"
      },
      "source": [
        "# Visualize model predictions against true values\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJArcI1O-wT1"
      },
      "source": [
        "# Plot the histogram of prediction errors\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4tj5tES-yIF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}