{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week07_LogisticRegression",
      "provenance": [],
      "authorship_tag": "ABX9TyNxpk/acYwBpD8tXDAeHEeq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ch00226855/CMP414765Spring2021/blob/main/Week07_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxoN-gYypjZk"
      },
      "source": [
        "# Week 7\r\n",
        "# Logistic Regression\r\n",
        "\r\n",
        "We have studied how to use linear regression and polynomial regression to *predict a target numeric value*. There is another learning task, **classification**, aiming at predicting group membership rather than numeric values. Email spam filter is a good example: it is trained with many example emails with their class (spam or non-spam), and it must learn how to classify new emails.\r\n",
        "\r\n",
        "Linear regression is **not** a good choice for classification tasks. We will introduce the **logistic regression** model and use the iris dataset to illustrate how the model works.\r\n",
        "\r\n",
        "**Readings4**: Textbook Chapter 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taAvtt3jppKN"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp3Dp0J5ppMc"
      },
      "source": [
        "## Logistic Regression: Intuition\r\n",
        "- Picture the data as points on the plane.\r\n",
        "- A classifier's job is to determine the decision regions for each class.\r\n",
        "- If a point is far from the decision boundary, then the classifier should be fairly confident about its prediction.\r\n",
        "- If a point is near the decision boundary, then the classifier may be less confident about its prediction.\r\n",
        "- The **logistic regression** model aims to provide a **probablity distribution** for each point. The probability distribution has little variance if the point is far from decision boundary.\r\n",
        "- **Probability distribution with high variance**: rolling a die - there is no way to predict the exact outcome\r\n",
        "- **Probability distribution with low variance**: getting the flu today - probably not going to happen\r\n",
        "\r\n",
        "<img src=\"https://mlr-org.com/docs/2015-07-28-Visualisation-of-predictions_files/figure-html/qda-1.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP_rRpctppO6"
      },
      "source": [
        "## Basic Case: Binary Classifier\r\n",
        "- Suppose there are only two classes for the output feature: **Class 0** (the negative class) and **Class 1** (the positive class).\r\n",
        "- A **binary classifer** tries to estimate the probability $p$ that a point belongs to Class 1.\r\n",
        "- The probability that a point belongs to Class 0 is $1 - p$.\r\n",
        "- Given the probability, the binary classifier will compare it with a chosen **threshold** (for example, 0.5), and then predict the class as\r\n",
        "    - prediction = 1 if $\\hat{p}$ $\\ge$ threshold\r\n",
        "    - prediction = 0 if $\\hat{p}$ < threshold\r\n",
        "- The **boundary** of decision regions is given by the curve formed by points whose probability equals to the threshold value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6kkVoPxppRZ"
      },
      "source": [
        "## Example: The Iris Dataset\r\n",
        "\r\n",
        "**Iris dataset** is a famous dataset that contains the sepal and petal length and width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica. [wiki page](https://en.wikipedia.org/wiki/Iris_flower_data_set)\r\n",
        "\r\n",
        "- Import dataset using <code>sklearn.dataset.load_iris()</code>\r\n",
        "- Explore the dataset: data description, feature names, data types, data histograms, scatter plots.\r\n",
        "- Split the dataset into train_set and test_set\r\n",
        "- Apply <code>sklearn.linear_model.LogisticRegression</code> to build a binary classifier on **Iris-Virginica**.\r\n",
        "- Evaluate the performance of the model: Accuracy, cross-validation, precision vs. recall, confusion matrix...\r\n",
        "- Visualize the model (show decision boundary)\r\n",
        "\r\n",
        "<img src=\"https://miro.medium.com/max/1000/1*Hh53mOF4Xy4eORjLilKOwA.png\" width=\"600\">\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "661QKVElppTu"
      },
      "source": [
        "# Load the dataset\r\n",
        "from sklearn import datasets\r\n",
        "iris = datasets.load_iris()\r\n",
        "\r\n",
        "iris.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR_7VatWppWA"
      },
      "source": [
        "# Description of the dataset\r\n",
        "print(iris['DESCR'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9I8J0yKppYd"
      },
      "source": [
        "print(iris['feature_names'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv0UuvJQppbE"
      },
      "source": [
        "# Convert the data into a data frame\r\n",
        "iris_df = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\r\n",
        "iris_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3sutAZbppdx"
      },
      "source": [
        "# Add the target class\r\n",
        "iris_df['target'] = iris['target']\r\n",
        "iris_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAuak-RkppgZ"
      },
      "source": [
        "# Explore the dataset\r\n",
        "# How many examples are there for each type of Iris?\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-CL5tdlqxpX"
      },
      "source": [
        "# Find the min, max, mean, and median of each variable\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x29VUGKNppim"
      },
      "source": [
        "# Create a function that maps 0-2 to the actual type of iris\r\n",
        "def get_target_name(x):\r\n",
        "    return iris['target_names'][x]\r\n",
        "\r\n",
        "x = iris_df.loc[124, 'target']\r\n",
        "name = get_target_name(x)\r\n",
        "print(x, name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8ccjhHEpplM"
      },
      "source": [
        "# Apply get_target_name() to all target values\r\n",
        "iris_df['target_name'] = iris_df['target'].apply(get_target_name)\r\n",
        "iris_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rh5H4fGppn2"
      },
      "source": [
        "# Draw scatter plots.\r\n",
        "plt.scatter(iris_df.loc[:, 'sepal length (cm)'], iris_df.loc[:, 'sepal width (cm)'], c=iris_df['target'])\r\n",
        "plt.colorbar()\r\n",
        "plt.xlabel(\"sepal length\")\r\n",
        "plt.ylabel(\"sepal width\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwE-j7aPppqX"
      },
      "source": [
        "# Draw all scatter plots\r\n",
        "from pandas.plotting import scatter_matrix\r\n",
        "scatter_matrix(iris_df.iloc[:, :4], figsize=(15, 15), marker='x',\r\n",
        "               c=iris_df['target'])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DbLrOEYpps9"
      },
      "source": [
        "## Build A Binary Classifier for Iris-Virginica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO4Wcfjappvi"
      },
      "source": [
        "# Define a function is_virginica(target) that returns 1 if target is Virginica\r\n",
        "# and 0 otherwise\r\n",
        "def is_virginica(target):\r\n",
        "\r\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edJ6tRU5ppyX"
      },
      "source": [
        "# Apply function is_virginica() to the data frame, creating a new \r\n",
        "# column \"Is_Virginica\"\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJYPukLVpp01"
      },
      "source": [
        "# Train-test split\r\n",
        "# Split the data frame into 85% training data and 15% test data\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiq_JIHkpp3X"
      },
      "source": [
        "# Display the amount of Virginica and non-Virginica cases in the training set\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6jq2Luspp8a"
      },
      "source": [
        "# Build the logistic regression model\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "model = LogisticRegression(solver='lbfgs')\r\n",
        "model.fit(df_train.iloc[:, :4], df_train['Is_Virginica'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj4tltViritD"
      },
      "source": [
        "## Model Evaluation\r\n",
        "- Classification accuracy\r\n",
        "- Cross Validation\r\n",
        "- Examine four categories using the confusion matrix:\r\n",
        "    - True Positive\r\n",
        "    - True Negative\r\n",
        "    - False Positive\r\n",
        "    - False Negative\r\n",
        "- Precision, recall, and F1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KT2AMrNrnfP"
      },
      "source": [
        "# 1. Find the prediction accuracy on test set\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU1HcHmrsDja"
      },
      "source": [
        "# 2. confusion matrix\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnlazoo8sKi3"
      },
      "source": [
        "<img src=\"https://hackernoon.com/hn-images/1*YV7zy1NGN1-HGQxY56nc_Q.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiXlzeYmsR6u"
      },
      "source": [
        "### 3. cross validation\r\n",
        "**Cross validation** is an efficient method that uses limited data to obtain multiple evaluations of the model.\r\n",
        "\r\n",
        "<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4788946%2F82b5a41b6693a313b246f02d79e972d5%2FK%20FOLD.png?generation=1608195745131795&alt=media\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGIB1iprtGt3"
      },
      "source": [
        "# Perform 3-fold cross validation\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "input_cols = iris_df.columns[:4]\r\n",
        "print(cross_val_score(model, df_train[input_cols], df_train['Is_Virginica'],\r\n",
        "                      cv=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLwO2VqftUAT"
      },
      "source": [
        "### 4. Precision, Recall, and F-1 Score\r\n",
        "**Precision** and **recall** are two important metrics that evaluates different aspects of the model. **F-1 score** is a combination of the precision and recall.\r\n",
        "\r\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/525px-Precisionrecall.svg.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3eFXvEstjiN"
      },
      "source": [
        "# precision - recall - f1 score\r\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\r\n",
        "precision = precision_score(df_test['Is_Virginica'], test_predictions) # How much Virigincia iris are correctly identified?\r\n",
        "recall = recall_score(df_test['Is_Virginica'], test_predictions) # How much Virginica predictions are correct?\r\n",
        "f1 = f1_score(df_test['Is_Virginica'], test_predictions)\r\n",
        "print(precision, recall, f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTSROe7Mt5te"
      },
      "source": [
        "## Logistic Regression: Model Assumption\r\n",
        "**Binary classifier model**: Logistic regression model assumes that the decision boundary is represented as a linear function:\r\n",
        "\r\n",
        "$\\log\\frac{\\hat{p}}{1 - \\hat{p}} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 +\\cdots + \\theta_nx_n,$\r\n",
        "- n: number of input features.\r\n",
        "- $x_1, ..., x_n$: input features\r\n",
        "- $\\hat{p}$: the estimated probability of data belonging to the class\r\n",
        "- $\\theta_1,...,\\theta_n$: parameters of the model\r\n",
        "\r\n",
        "**Alternative format**:\r\n",
        "\r\n",
        "$\\hat{p} = \\sigma(\\textbf{x}\\cdot\\theta^T).$\r\n",
        "\r\n",
        "- $\\textbf{x} = (1, x_1, ..., x_n)$.\r\n",
        "- $\\theta = (\\theta_0, \\theta_1, ..., \\theta_n)$.\r\n",
        "- $\\sigma(t) = \\frac{1}{1+e^{-t}}$: logistic function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anlBy0OauLaV"
      },
      "source": [
        "# Plot the graph of logistic function\r\n",
        "\r\n",
        "# 1. Pick a list of x coordinates (`np.linspace`)\r\n",
        "x = np.linspace(-10, 10, 100)\r\n",
        "# 2. For each x, find the value of the function\r\n",
        "values = 1 / (1 + np.exp(-x)) # Since x is a numpy array, we can apply\r\n",
        "                                # np.exp directly\r\n",
        "# 3. plot the list of x coordinates and y coordinates using\r\n",
        "plt.plot(x, values, )\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdQxqkj-uReO"
      },
      "source": [
        "## Logistic Regression: Decision Rule\r\n",
        "\r\n",
        "**Decision rule**: Pick a threshold (for example, 0.5), and then\r\n",
        "\r\n",
        "- prediction = 1 if $\\hat{p}$ $\\ge$ threshold\r\n",
        "- prediction = 0 if $\\hat{p}$ < threshold\r\n",
        "\r\n",
        "**Trade-off with threshold**:\r\n",
        "- If threshold is chosen closer to 1, then the positive predictions are __more likely__ to be correct (fewer **false positives**). However, the negative predictions are __less likely__ to be correct.\r\n",
        "- If threshold is chosen closer to 0, then the negative predictions are __more likely__ to be correct (fewer **false negatives**). However, the positive predictions are __less likely__ to be correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2RuK1ZfuUGO"
      },
      "source": [
        "## Logistic Regression: Cost Function and Training Algorithm\r\n",
        "For classification tasks, it is no longer appropriate to use MSE as the cost function.\r\n",
        "\r\n",
        "**Cost (loss) function** for logistic regression:\r\n",
        "\r\n",
        "\\begin{equation}\r\n",
        "c(\\theta) = \\left\\{\r\n",
        "\\begin{array}{cc}\r\n",
        "-\\log(\\hat{p}) & \\textit{if  }y=1,\\\\\r\n",
        "-\\log(1-\\hat{p}) & \\textit{if  }y=0.\r\n",
        "\\end{array}\r\n",
        "\\right.\r\n",
        "\\end{equation}\r\n",
        "\r\n",
        "The cost function $c(\\theta)$:\r\n",
        "\r\n",
        "- small if $y=1$ (data example belongs to the class) and $\\hat{p}$ is close to 1.\r\n",
        "- small if $y=0$ (data example does not belong to the class) and $\\hat{p}$ is close to 0.\r\n",
        "- is a convex function no matter what $y$ is.\r\n",
        "\r\n",
        "**Uniformed expression for the cost function**:\r\n",
        "\r\n",
        "$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}\\big[y^{(i)}\\log(\\hat{p}^{(i)}) + (1-y^{(i)})\\log(1-\\hat{p}^{(i)})\\big]$\r\n",
        "\r\n",
        "- $c(\\theta) = J(\\theta)$ for $y=0$ and $y=1$.\r\n",
        "- There is no equivalent of the Normal Equation.\r\n",
        "- $J(\\theta)$ is a convex function, so the *gradient descent algorithm* will guarantee to find its global minimum.\r\n",
        "- $\\frac{\\partial J}{\\partial \\theta_j}=\\frac{1}{m}\\sum_{i=1}^{m}\\big(\\sigma(\\textbf{x}^{(i)}\\cdot\\theta^T) - y^{(i)}\\big)x_j^{(i)}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbg7vKt8vRk-"
      },
      "source": [
        "## Logistic Regression: Model Visualization\r\n",
        "- Create a grid of points from a list of x coordinates and y coordinates.\r\n",
        "- Use the model to obtain prediction probability on each point from the grid\r\n",
        "- Find points with marginal probabilities.\r\n",
        "- Plot the grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mUb3gXVvzaj"
      },
      "source": [
        "# Train a new logistic regression model on petal length and petal width only\r\n",
        "model2 = LogisticRegression(solver='lbfgs')\r\n",
        "model2.fit(df_train.iloc[:, 2:4], df_train['Is_Virginica'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "578Aekyrv1Jp"
      },
      "source": [
        "# 1. Create a grid of points\r\n",
        "x0, x1 = np.meshgrid(np.linspace(0, 7, 100),\r\n",
        "                     np.linspace(0, 2.7, 100))\r\n",
        "print(x0.shape, x1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMkv4u-Lv2_y"
      },
      "source": [
        "# Illustration of a meshgrid\r\n",
        "x_coordinates = [1, 2, 3, 4]\r\n",
        "y_coordinates = [10, 20, 30, 40]\r\n",
        "xx, yy = np.meshgrid(x_coordinates, y_coordinates)\r\n",
        "# print(xx)\r\n",
        "# print(yy)\r\n",
        "plt.plot(xx, yy, 'b.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIopn3Sgv-Ec"
      },
      "source": [
        "# 2. Obtain prediction probabilities\r\n",
        "X_new = np.hstack([x0.reshape([-1, 1]), x1.reshape([-1, 1])])\r\n",
        "y_new_prob = model2.predict_proba(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f9UF-20wDDS"
      },
      "source": [
        "# 3. Find boundary points.\r\n",
        "# Which points give 0.5 probability?\r\n",
        "indices = np.where((y_new_prob[:, 1] > 0.49) & (y_new_prob[:, 1] < 0.51))\r\n",
        "X_boundary = X_new[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3GzSTg_wENt"
      },
      "source": [
        "# 4. Plot the boundary\r\n",
        "plt.plot(X_boundary[:, 0], X_boundary[:, 1])\r\n",
        "index_virginica = (iris_df['Is_Virginica'] == 1)\r\n",
        "index_not_virginica = (iris_df['Is_Virginica'] == 0)\r\n",
        "plt.scatter(iris_df.loc[index_virginica, 'petal length (cm)'],\r\n",
        "            iris_df.loc[index_virginica, 'petal width (cm)'],\r\n",
        "            c='yellow',\r\n",
        "            label='Virginica')\r\n",
        "plt.scatter(iris_df.loc[index_not_virginica, 'petal length (cm)'],\r\n",
        "            iris_df.loc[index_not_virginica, 'petal width (cm)'],\r\n",
        "            c='purple',\r\n",
        "            label='Not Virginica')\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GowAikpPwGUN"
      },
      "source": [
        "# 5. Plot probabilities\r\n",
        "plt.scatter(X_new[:, 0], X_new[:, 1], c=y_new_prob[:, 0])\r\n",
        "plt.colorbar()\r\n",
        "plt.scatter(iris_df.loc[index_virginica, 'petal length (cm)'],\r\n",
        "            iris_df.loc[index_virginica, 'petal width (cm)'],\r\n",
        "            c='yellow',\r\n",
        "            label='Virginica')\r\n",
        "plt.scatter(iris_df.loc[index_not_virginica, 'petal length (cm)'],\r\n",
        "            iris_df.loc[index_not_virginica, 'petal width (cm)'],\r\n",
        "            c='purple',\r\n",
        "            label='Not Virginica')\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlpThSQtwHyD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}